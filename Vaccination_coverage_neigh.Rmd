---
title: "Vaccination_coverage"
author: "S.W.F.  van Rijk"
date: "1-4-2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 

```{r cars}
getwd()
```



```{r Packages, include=FALSE}
if (!require("easypackages")) install.packages("easypackages") #easy package manager

#Set up and data analysis packages
if (!require("tidyverse")) install.packages("tidyverse") #main data science packages

#Some GIS other analyses packages 
if (!require("sf")) install.packages("sf") #main GIS package
if (!require("sp")) install.packages("sp") #needed for some GIS operation, will not be in use from 2023
if (!require("spdep")) install.packages("spdep") #neighborhood analysis in R
if (!require("spatialreg")) install.packages("spatialreg") #spatial modelling such as lag, error
if (!require("spgwr")) install.packages("spgwr") #GWR modelling
if (!require("RColorBrewer")) install.packages("RColorBrewer") # getting interesting color
if (!require("tmap")) install.packages("tmap") # Mapping package
if (!require("mapview")) install.packages("mapview") # Mapping package
if (!require("car")) install.packages("car") #some base regression functions
if (!require("cowplot")) install.packages("cowplot") #some base regression functions
if (!require("leafsync")) install.packages("leafsync") #using with mapview
if (!require("leaflet.extras2")) install.packages("leaflet.extras2") #using with mapview
if (!require("reshape2")) install.packages("reshape2") #using with mapview

#load libraries

easypackages::packages ("sf", "sp", "spdep", "spatialreg", "spgwr", "tmap", "mapview", "car", "RColorBrewer", "tidyverse", 
                        "cowplot", "leafsync", "leaflet.extras2", "mapview")
library(readxl)
library(sqldf)
library(ISOweek)
library(sf)
#clean your environment
rm(list=ls())
```

## Reading polygons

```{r}

neigh <- st_read("data/wijken_onshore.gpkg", layer = "wijken_onshore")

neigh <- neigh[,c("WK_CODE","WK_NAAM","GM_CODE","GM_NAAM","Shape_Area","geom")]

# Map
neigh_map <- mapview::mapview(neigh,
                              col.regions=rev(brewer.pal(11, "RdYlGn")),
                              layer.name = 'Neighborhoods'
                              )
#neigh_map

```

## Read data

```{r}

# neighborhood demographic data
neigh_data <- read_excel("data/kwb-2020.xls", sheet = "KWB2020")
# metadata
neigh_meta <- read_excel("data/kwb-variables.xlsx", sheet="kwb-variables")

# filtering
neigh_data <- neigh_data %>% subset(recs=='Wijk') # filter to only 'wijk' regions
neigh_data[,7:119] <- lapply(neigh_data[,7:119], gsub, pattern = ",", replacement = ".") # comma separators to dots
neigh_data[,7:119] <- neigh_data[,7:119] %>% mutate_if(is.character,as.numeric) # set variables to numeric

var_to_keep <- (neigh_meta %>% subset(Include==1))$Variabelenaam # subset to variables to keep
neigh_data <- neigh_data %>% select(var_to_keep) # filter data to variables to keep

# normalizing
var_to_norm <- neigh_meta %>% subset(`Need to normalise`==1 & Include==1) %>% select(c('Variabelenaam','Normalisatie'))

for(row in 1:nrow(var_to_norm)){
  var <- var_to_norm[row,'Variabelenaam'][[1]]
  norm <- var_to_norm[row,'Normalisatie'][[1]]
  neigh_data[,var] <- neigh_data[,var]/neigh_data[,norm]
}


# neighborhood vaccination data
vaccine_neigh <- read_delim("data/COVID-19_vaccinatiegraad_per_wijk_per_week.csv", delim=";")
vaccine_neigh[,10:11] <- vaccine_neigh[,10:11] %>% mutate_if(is.character,as.numeric) # set variables to numeric

vaccine_neigh[vaccine_neigh == 9999] <- NA # make hidden missing values no longer hidden

vaccine_neigh <- vaccine_neigh %>% 
  subset(Date_of_statistics=='14-3-2022') %>% 
  mutate(coverage_partly_norm = ((Coverage_primary_partly) - mean(Coverage_primary_partly,na.rm=TRUE)) / sd(Coverage_primary_partly,na.rm=TRUE),
         coverage_completed_norm= ((Coverage_primary_completed) - mean(Coverage_primary_completed, na.rm=TRUE))/sd(Coverage_primary_completed, na.rm = TRUE))


# check normalisation
mean(vaccine_neigh$coverage_completed_norm, na.rm=TRUE)
sd(vaccine_neigh$coverage_completed_norm, na.rm=TRUE)

mean(vaccine_neigh$coverage_partly_norm, na.rm=TRUE)
sd(vaccine_neigh$coverage_partly_norm, na.rm=TRUE)

# national coverage values
mpart <- round(mean(vaccine_neigh$Coverage_primary_partly, na.rm=TRUE),digits=2)
spart <- round(sd(vaccine_neigh$Coverage_primary_partly, na.rm=TRUE),digits=2)

mfull <- round(mean(vaccine_neigh$Coverage_primary_completed, na.rm=TRUE),digits=2)
sfull <- round(sd(vaccine_neigh$Coverage_primary_completed, na.rm=TRUE),digits=2)




# histogram of vaccination status

vaccine_neigh %>% ggplot() +
    # partly vaccinated
    geom_histogram(aes(x = Coverage_primary_partly), fill = 'red', alpha = 0.5, bins=75, na.rm = TRUE) + 
    geom_density(aes(x = Coverage_primary_partly,y = ..count..), col='red',size=1,na.rm = TRUE) + 
    
    # fully vaccinated
    geom_histogram(aes(x = Coverage_primary_completed), fill = "blue", alpha = 0.5,bins=75, na.rm = TRUE) + 
    geom_density(aes(x = Coverage_primary_completed, y = ..count..),col='blue',size=1,na.rm = TRUE) +
    # means
    geom_vline(aes(xintercept=mean(Coverage_primary_completed, na.rm=TRUE),col='blue'), linetype="dashed", size=1) +
    geom_vline(aes(xintercept=mean(Coverage_primary_partly, na.rm=TRUE),col='red'), linetype="dashed", size=1) +
    # formatting
    labs(title='Vaccination against SARS-CoV-2 coverage on 14-3-2022') +
    ylab('Number of neighbourhoods') +
    xlab('Percentage') + 
    # legend
    scale_colour_manual(name="Coverage type", values=c("blue", "red"), labels=c(paste(mfull,'% (',sfull,'%)'), paste(mpart,'% (',spart,'%)')))

```

## Delete outlier neighborhoods?


```{r}
# Delete where a_inw == 0, causes NaNs and Inf numbers through divide by zero
neigh_data_filt <- neigh_data %>% filter(a_inw != 0)

# maybe remove all neigh lower than 50 people?
# neigh_data_filt <- neigh_data %>% filter(a_inw >= 50)

# select only the vars used in the model, to remove as little observations as possible with complete cases

neigh_data_filt <- neigh_data_filt %>% select(
    gwb_code,
    a_inw,
  # gender
    a_man,
    a_vrouw,
  # age groups
    a_00_14,
    a_15_24,
    a_25_44,
    a_45_64,
    a_65_oo,
  # ethnic groups
    a_w_all,
    a_nw_all,
    a_marok,
    a_antaru,
    a_suri,
    a_tur,
    a_ov_nw,
  # household
    p_geb,
    p_ste,
    a_hh,
    #a_1p_hh,
    a_hh_z_k,
    #a_hh_m_k,
    g_hhgro,
    bev_dich,
    p_1gezw,
    p_mgezw,
  # education
    a_opl_lg,
    a_opl_md,
    a_opl_hg) %>% 
  mutate_at(vars(-("gwb_code")), ~(scale(.) %>% as.vector)) # z-normalization

```



## Join datasets

```{r}

# FULL OUTER JOINS 
# take into accounts for models, some columns have NA !


data_compl <- sp::merge(x = vaccine_neigh, 
                        y = neigh_data_filt,
                        by.x = 'Region_code',
                        by.y = 'gwb_code',
                        all = FALSE)

data_compl <- data_compl[complete.cases(data_compl),]

data_compl <- sp::merge(x = neigh,
                        y = data_compl,
                        by.x = 'WK_CODE',
                        by.y = 'Region_code',
                        all = FALSE)





vaccine_map <- mapview::mapview(data_compl,
                                zcol = 'Coverage_primary_completed',
                                col.regions=brewer.pal(11, "RdYlGn"),
                                layer.name = 'Vaccination coverage'
                                )
#vaccine_map

vaccine_norm_map <- mapview::mapview(data_compl,
                                zcol = 'coverage_completed_norm',
                                col.regions=brewer.pal(11, "RdYlGn"),
                                layer.name = 'Vaccination coverage z-score normalized'
                                )
vaccine_norm_map


```
## Correlation

```{r}
library(mice)

md.pattern(data_compl)





test_var <- data_compl %>% select_if(is.numeric) %>% names()
test_var <- test_var[-length(test_var)]
test_var <- test_var[6:length(test_var)]

corr <- cor(as.data.frame(data_compl)[,test_var],use="complete.obs")

library(corrplot)

corrplot(corr, order = "hclust",method="color", tl.cex=0.5,tl.col = 'black')



```



## Linear model and Morran's I

```{r}

# basic equation
lm_eq1 <- coverage_partly_norm ~ 
    #a_inw +
  # gender
    #a_man + 
    #a_vrouw + 
  # age groups
    #a_00_14 +
    a_15_24 +
    #a_25_44 +
    #a_45_64 +
    a_65_oo +
  # ethnic groups
    a_w_all  +
    a_nw_all +
    #a_marok +
    #a_antaru +
    #a_suri +
    #a_tur +
    #a_ov_nw +
  # household
    p_geb +
    #p_ste +
    a_hh +
    #a_1p_hh +
    #a_hh_z_k +
    #a_hh_m_k +
    #g_hhgro +
    #bev_dich +
    #p_1gezw +
    #p_mgezw +
  # education
    a_opl_lg +
    #a_opl_md +
    a_opl_hg



#run the model
linearMod <- lm (lm_eq1, data = data_compl)

#get summary
summary(linearMod)

```
```{r}

vif(linearMod)

# high multicollinearity between the variables.

```

```{r, equal spatial weights}

#creating adjacency matrix, this uses equal distributed weights for only direct neighbours

data_nbq <- poly2nb(data_compl, queen=TRUE) #Queen’s Contiguity neighborhood
summary(data_nbq)

data_nbq_w <- nb2listw(data_nbq, style="W", zero.policy = TRUE) #Queen’s neighborhood weights
summary(data_nbq_w, zero.policy = TRUE)

#for plotting
coordsW <- data_compl%>%
  st_centroid()%>%
  st_geometry()

plot(data_nbq, st_geometry(coordsW), col="red")

```

```{r, inverse distance weights, eval=F, echo=F}
          # knitr DISABLED!           ^       ^

#creating inverse distance matrix (euclidean)

coords = st_centroid(data_compl)

nb_knn = knn2nb(knearneigh(coords, k = 20)) # picked k=20 because there are 2 neigh that have 20 neigh in queen's cont.
#nb_knn = dnearneigh(coords, 0, 5000) # picked d=5000 since this is the ~mean dist of the adjacency matrix
dist <- nbdists(nb_knn, coords, longlat = TRUE) #distance weights matrix

alpha = 1 #using alpha = 1 because expecting the weight importance to fall off not so fast
#ids <- lapply(dist, function(x) 1/(alpha * x)) # inverse distance calculation
meandist = mean(unlist(dist)) # # dividing all x by mean distance, to prevent exp to become too small to calculate
ids <- lapply(dist, function(x) exp(-alpha * x/meandist))

data_nbq_w <- nb2listw(nb_knn, glist=ids, style="W", zero.policy = TRUE) #row normalize weights
summary(data_nbq_w, zero.policy = TRUE)

#for plotting
coordsW <- data_compl%>%
  st_centroid()%>%
  st_geometry()

plot(data_nbq, st_geometry(coordsW), col="red")

```

```{r}
#Usually we use the moran.test function to get the Moran’s I, but that method is sensitive to irregularly distributed polygons. So we would use Monte Carlo method to bootstrap different polygon distribution. Here moran.mc() funtion does the job

mc_global <- moran.mc(linearMod$residuals, data_nbq_w, 20, alternative="greater", zero.policy = TRUE,  adjust.n = TRUE)

#plot the  Moran’s I
plot(mc_global)


#Now plot the residual on the polygon
data_compl$res_lm <- residuals(linearMod)
lmres <- qtm(data_compl, "res_lm", fill.style="fixed", fill.breaks=c(-6:3), borders = NULL)
lmres


```

## GWR model on neighborhoods
```{r, GWR, eval=FALSE, echo=FALSE}

#1 - Testing spatial lag model
spa_lagmodel = lagsarlm (lm_eq1, data = data_compl, listw= data_nbq_w, zero.policy = TRUE)

summary(spa_lagmodel, Nagelkerke=T)
```

```{r, residual, eval=FALSE, echo=FALSE}
#check residual autocorrelation

mc2_global <-moran.mc(spa_lagmodel$residuals, data_nbq_w, 2999, alternative="greater", zero.policy = TRUE,  adjust.n = TRUE)

plot(mc2_global)
```

```{r, plot, eval=FALSE, echo=FALSE}
#add the residual to polygon and plot
data_compl$res_slm <- residuals(spa_lagmodel)

#plot using t-map
slmres <-qtm(data_compl, "res_slm", fill.style="fixed", fill.breaks=c(-6:4), borders = NULL)

#compare with OLS residual
tmap_arrange(lmres, slmres, asp = 1, ncol = 2)
```

```{r}
#Before we do any analysis for GWR we need to convert our importanted sf object into a sp spatial object. As the spgwr package usually works on sp objects
#converting the sf polygon into sp object

data_sp <- as_Spatial(data_compl)
```


```{r, gwr, eval = FALSE, echo = FALSE}
#find the optimum bandwidth distance for fixed kernel using the gwr.sel() function from spgwr package
# fbw <- gwr.sel(lm_eq1,
#               data = data_sp,
#               longlat = TRUE,
#               adapt=FALSE,
#               gweight = gwr.bisquare,
#               verbose = T)

fbw = 5414.076 # found with function above


```

```{r, eval = FALSE, echo = FALSE}
fb_gwr <- gwr(lm_eq1, 
              data = data_sp,
              longlat = TRUE,
              bandwidth = fbw, 
              gweight = gwr.bisquare,
              hatmatrix=TRUE, 
              se.fit=TRUE)

#summary of the model
fb_gwr
```

```{r, eval = FALSE, echo = FALSE}
#Extract the modeled relations for gwr object
fb_gwr_out <- as.data.frame(fb_gwr$SDF)

#see the data frame, there is 1673 regressions, each row is a regression result
#view(fb_gwr_out)


#join that with our main polygon data frame for the R2
data_compl$fmb_localR2 <- fb_gwr_out$localR2

mapview::mapview(data_compl, zcol = "fmb_localR2", col.regions=brewer.pal(11, "RdYlGn"))
```
```{r}
ggplot(data = melt(fb_gwr_out[,2:10]), aes(x=variable, y=value)) + 
  geom_boxplot() + 
  xlab('Variable coefficient') + 
  ylab('Estimate') +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
  geom_hline(yintercept = 0) + theme(axis.text = element_text(size=16),axis.title = element_text(size=16))

#ggsave(file=paste0(getwd(),"/figures/gwr_var_nei.png"),width = 7, height = 10)
```

```{r, plotting, eval = FALSE, echo = FALSE}
#join that with our main polygon data frame for high education
data_compl$fcoef_a_opl_hg = fb_gwr_out$a_opl_hg

coef_fb_gav <- mapview::mapview(data_compl, zcol = "fcoef_a_opl_hg")

coef_fb_gav
```

```{r, adaptive kernal}
#adaptive kernel
#abw <- gwr.sel (lm_eq1, 
#              data = data_sp,
#              adapt = TRUE, 
#              gweight = gwr.bisquare)

abw = 0.07149476 # precalculated by above function

#Fitting the adaptive Kernel GWR
ab_gwr <- gwr(lm_eq1, 
              data = data_sp,
              longlat = TRUE,
              adapt = abw, 
              gweight = gwr.bisquare,
              hatmatrix=TRUE, 
              se.fit=TRUE)

#summary of the model
ab_gwr
```


```{r, gwr->DF}
#adaptive GWR results in data frame
ab_gwr_out <- as.data.frame(ab_gwr$SDF)

#join the local R2 for each model to each neighborhood
data_compl$amb_localR2 <- ab_gwr_out$localR2

mapview::mapview(data_compl, zcol = "amb_localR2", col.regions=brewer.pal(11, "RdYlGn")) 


```

```{r,variability}
#join that with our main polygon data frame for coverage availability
data_compl$fcoef_a_opl_hg = ab_gwr_out$a_opl_hg

coef_ab_gav <- mapview::mapview(data_compl, zcol = "fcoef_a_opl_hg")

coef_ab_gav
```

```{r,significance fb, eval = FALSE, echo = FALSE}



#For fixed model
#estimate the t-value for high education variable for fixed kernel model
data_compl$ft_a_opl_hg = fb_gwr_out$a_opl_hg / fb_gwr_out$a_opl_hg_se

#categorize the t-value to statistical significance
data_compl$ft_a_opl_hg_cat <- cut(data_compl$ft_a_opl_hg,
                             breaks=c(min(data_compl$ft_a_opl_hg), -1.96, 1.96, max(data_compl$ft_a_opl_hg)),
                             labels=c("sig","nonsig", "sig"))

#plot the significance for fixed kernel model
opl_hg_sig_fb <- mapview::mapview (data_compl, zcol = "ft_a_opl_hg_cat")

opl_hg_sig_fb


```


```{r,significance ab}
#For Adaptive model
#estimate the t-value for opl_hg_ space availability variable for fixed kernel model
data_compl$at_a_opl_hg = ab_gwr_out$a_opl_hg / ab_gwr_out$a_opl_hg_se

#categorize the t-value to statistical significance
data_compl$at_a_opl_hg_cat <- cut(data_compl$at_a_opl_hg,
                             breaks=c(min(data_compl$at_a_opl_hg), -1.96, 1.96, max(data_compl$at_a_opl_hg)),
                             labels=c("sig","nonsig", "sig"))

#plot the significance for adaptive kernel model
opl_hg_sig_ab <- mapview::mapview(data_compl, zcol = "at_a_opl_hg_cat")


#compare the maps
#opl_hg_sig_ab | opl_hg_sig_fb # doesn't seem to work so plot separately
opl_hg_sig_ab

```

```{r}
sigPlot <- function(var,gwr_out){
  #estimate the t-value for green space availability variable for fixed kernel model
  t = gwr_out[,var] / gwr_out[,paste0(var,'_se')]

  #categorize the t-value to statistical significance
  data_compl[,paste0(var,'_cat')] <- cut(t,
                             breaks=c(min(t), -1.96, 1.96, max(t)),
                             labels=c("sig","nonsig", "sig"))
  
  qtm(data_compl, paste0(var,'_cat'), border = NULL)
  
}


sigPlot('a_hh',ab_gwr_out)
sigPlot('a_w_all',ab_gwr_out)
#sigPlot('a_opl_hg',ab_gwr_out)
#sigPlot('a_hh',ab_gwr_out)
#sigPlot('a_nw_all',fb_gwr_out)
```


```{r}
ggplot(data = melt(ab_gwr_out[,2:10]), aes(x=variable, y=value)) + 
  geom_boxplot() + 
  xlab('Variable coefficient') + 
  ylab('Estimate') +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
  geom_hline(yintercept = 0) + theme(axis.text = element_text(size=16),axis.title = element_text(size=16))

#ggsave(file=paste0(getwd(),"/figures/gwr_var_mun.png"),width = 7, height = 10)
```

